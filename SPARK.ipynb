{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data\n",
    "* Datasets that are so large or so complex that traditional data processing software or simple code is inadequate to deal with them.\n",
    "* Not only data, but also the infrastructure that is needed to support such analyses.\n",
    "* Four Vs\n",
    "    * Volume - \"Scale of Data\" - Generally, anything bigger than 1GB of data is considered to be 'BIG'\n",
    "    * Velocity - \"Speed of Data\"\n",
    "    * Variety - \"Diversity of Data\" Could be dense or sparse, or dependent on time, or not, different types of data.\n",
    "    * Veracity - \"The certainty of data\" Is the data trustworthy/valid ?\n",
    "    \n",
    "Always use data processing tools with discretion i.e. If you only have a dataset that is .1 mb, you should probably stick to something like CSV.\n",
    "\n",
    "Searching is typically spread across many machines.\n",
    "\n",
    "To store, process, and recall information from large and complex data sets, it's almost always a necessity to have more than one computer, or relatively small size server, to handle the data. When you start to have data spread across, potentially, many machines, you need to have tools that abstract away the management and work flow needed to use multiple machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of data you will find in the wild\n",
    "* Structured Data - makes most sense for relational databases. i.e. Pandas Dataframes\n",
    "* Unstructured Data - Images, Audio, video, GeoSpatial, etc.\n",
    "\n",
    "### 3 Major domains of big data\n",
    "1. Computations\n",
    "2. Infrastructure\n",
    "3. Data Storage\n",
    "\n",
    "### Processing Data \"Computations use cases for big data\"\n",
    "Generalized Data Processing - Being fed data and running a computation over it.\n",
    "Hadoop, Apache Spark \"built on top of Hadoop\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark - Distributed data processing framework\n",
    "* Based on resilient distributed datasets\n",
    "RDD - A huge list of objects in memory, which is so big it needs to be distributed across many computers.'\n",
    "\n",
    "* Spark gives you shared variables\n",
    "\n",
    "Functional tools in Python needed for Spark (Perform functions on list, and return another list)\n",
    "* lambda\n",
    "    * Anonymous functions lambda x: x + 1\n",
    "* map\n",
    "    * Apply a function to each item in a list, and return a new list with the applied variables (Like apply in Pandas)\n",
    "    * map(add1, [1, 2, 3]) => [2, 3, 4]\n",
    "* filter\n",
    "    * filter(isOdd, [1, 2, 3, 4]) = > [1, 3]\n",
    "* reduce\n",
    "    * Applies a function to all pairs of elements of a list, returns ONE value not a list\n",
    "    * reduce(add, [1, 2, 3, 4]) => 10 \"(((1 + 2) + 3) + 4)\"\n",
    "* itertools\n",
    "    * Chain\n",
    "    * FlatMap\n",
    "        * Flattens a list of lists. i.e. [[1, 2, 3, 4], [7, 8, 9]]\n",
    "        * from itertools import chain\n",
    "        * chain(map(lambda t: range(t[0], t[1]), [(1,5), (7,9)])) => [1, 2, 3, 4, 7, 8, 9]\n",
    "        \n",
    "### In the Spark Shell \"pyspark\"\n",
    "* sc.parallelize => Takes a list of elements, and sends it to Spark to return an RDD\n",
    "* writing sc.parallelize doesn't actually do anything, it just tells Spark \"if I ever need to run this, I will run it this way\".\n",
    "\n",
    "We divide RDD methods into two kinds:\n",
    "1. Transformations (Map, Filter, FlatMap)\n",
    "* Return another RDD\n",
    "* Are not really performed, until an action is called (lazy)\n",
    "2. Actions (Reduce, Take, Collect, Sum)\n",
    "* Return a value other than an RDD\n",
    "* Are performed immediately\n",
    "\n",
    "Example = RDD1 = sc.parallelize( range(1, 1000) )\n",
    "RDD1.map(lambda x: x + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text Files\n",
    "sc.textFile(path, min_partitions, useUnicode=True)\n",
    "* Returns an RDD of strigns (1 per line)\n",
    "* Can read many files using wildcard, *\n",
    "* Can read from hdfs, ....\n",
    "Example = people = sc.textFile(\"../data/people.txt\")\n",
    "\n",
    "### Group By Functions\n",
    "1. Read in File, lets say one structured  \"Name | Gender | Age\"\n",
    "2. We use reduceByKey to group by certain criteria\n",
    "Example = people.map(lambda t: (t[1], 1)).reduceByKey(lambda x, y: x + y) => [(M, 3), (F, 5)]\n",
    "3. You are grouping by the second column t[1] Gender, and summing up the count of each one.\n",
    "\n",
    "\n",
    "Code in Apache Spark travels from Scala to python interpreter for things like map/reduce\n",
    "\n",
    "### Create objects in python\n",
    "1. Load .py files that has your class in it, and import the class\n",
    "from something.py import Person\n",
    "2. people = sc.textFile(\"../../data/sales/sales.txt\").map(lambda x: Person().parse(x))\n",
    "3. Outputs people objects\n",
    "\n",
    "### Joins \n",
    "* You can use Joins to combine two RDDs\n",
    "\n",
    "states = [(\"AK\", \"Alaska\"), (\"AL\", \"Alabama\"), (\"AZ\", \"Arizona\")] \n",
    "populations = [(\"AK\": 100,000), (\"AL\", 90,000), (\"AZ\", 345,000)]\n",
    "\n",
    "states_rdd = sc.parallelize(states)\n",
    "populations_rdd = sc.parallelize(populations)\n",
    "\n",
    "states_rdd.join(populations_rdd);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a Spark Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we write a Spark program, we write a driver program. We describe to Spark the sequence of operations we want applied to our data, and the Spark driver program is than going to orchestrate whats going to happen on the worker nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame\n",
    "img here\n",
    "\n",
    "### Specify Feature Extraction\n",
    "img here\n",
    "\n",
    "### Model Evaluation\n",
    "img here\n",
    "\n",
    "SIDENOTE* = Good way of solving the problem of overfitting with decision trees is using random forests. Decision Trees have a tendency to get really specific on the training set, but really bad on unknown data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
