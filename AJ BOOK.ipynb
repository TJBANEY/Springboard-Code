{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised - You have labeled output\n",
    "    * We have inputs (age, year, time) and outputs (money, stock value, gender)\n",
    "    \n",
    "Regression - Predicting continuous values\n",
    "    * Linear Regression\n",
    "        * Stepwise Regression\n",
    "        * Ridge Regression\n",
    "        * Principal Component Regression\n",
    "        * Partial Least Square \n",
    "        * Lasso\n",
    "\n",
    "Classification - Predicting classes\n",
    "    * Tree Based Methods\n",
    "        * Bagging\n",
    "        * Boosting \n",
    "        * Random Forests\n",
    "        \n",
    "========================================================================================\n",
    "\n",
    "Unsupervised - You don't have labeled output\n",
    "    * There are inputs but no outputs, we have to look for patterns in the data using methods like 'clustering' and are NOT trying to predict output values.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Discriminant Analysis Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component - If a dataset has thousands of features, it is best to find only the most important feature that explain most of the data's variance. These are the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History\n",
    "1. Early 19th century, Legendre and Gauss develop the idea of 'Least Squares' which implemented the earliest form of what is now known as 'linear regression'\n",
    "\n",
    "2. 1936, Fisher proposed Linear Discriminant Analysis\n",
    "\n",
    "3. 1940, Logistic regression comes out as alternative approach\n",
    "\n",
    "4. 1970, More techniques for learning from data come out, but most are linear because fitting non-linear relationships were computationslly infeasible.\n",
    "\n",
    "5. 1980 - Breiman, Friedman, Olshen and Stone - come out with Regression Trees and Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Statistical learning should not be viewed as a series of black boxes. No\n",
    "single approach will perform well in all possible applications. Without\n",
    "understanding all of the cogs inside the box, or the interaction\n",
    "between those cogs, it is impossible to select the best box. Hence, we\n",
    "have attempted to carefully describe the model, intuition, assumptions,\n",
    "and trade-offs behind each of the methods that we consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*While it is important to know what job is performed by each cog, it\n",
    "is not necessary to have the skills to construct the machine inside the\n",
    "box! Thus, we have minimized discussion of technical details related\n",
    "to fitting procedures and theoretical properties. We assume that the\n",
    "reader is comfortable with basic mathematical concepts, but we do\n",
    "not assume a graduate degree in the mathematical sciences. For instance,\n",
    "we have almost completely avoided the use of matrix algebra,\n",
    "and it is possible to understand the entire book without a detailed\n",
    "knowledge of matrices and vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Chapter 2 \n",
    "\n",
    "Reducible Error\n",
    "Irreducible Error\n",
    "\n",
    "Predictions - Finding a model that will most accurately predict response\n",
    "    * Given A, B, and C what will this person's salary be ?\n",
    "Inference - What way is Y affected as X changes ?\n",
    "    * Feature Selection = Which features are actually associated with Y ?\n",
    "    \n",
    "    \n",
    "2 Approaches to creating a function for predictions\n",
    "Parametric\n",
    "    * Reduces the problem of estimating prediction function, F(x) down to one of estimating a set of parameters \"PARAMETER - ETRIC\"\n",
    "    * Find the coefficients of the function B1, B2, B3\n",
    "Non-Parametric\n",
    "\n",
    "The most common way to fitting a model is ordinary least squares\n",
    "\n",
    "OVERFITTING = Function, F(x), used for predictions will follow noise to closely, and will not be an accurate match for the true function of x.\n",
    "\n",
    "Any parametric approach brings with it the possibility that the functional form used to estimate f is very different from the true f, in which case the resulting model will not fit the data well. In contrast, non-parametric approaches completely avoid this danger\n",
    "\n",
    "There is a TRADE OFF between interpretability, and flexibility. As algorithms become more flexible, they become less interpretable. We want things to be interpretable for inference problems, but don't care for predictions.\n",
    "\n",
    "Mean Squared Error = Way to measure the quality of a fit.\n",
    "\n",
    " When\n",
    "a given method yields a small training MSE but a large test MSE, we are\n",
    "said to be overfitting the data. This happens because our statistical learning\n",
    "procedure is working too hard to find patterns in the training data, and\n",
    "may be picking up some patterns that are just caused by random chance\n",
    "rather than by true properties of the unknown function f.\n",
    "\n",
    "What do we mean by the variance and bias of a statistical learning\n",
    "method? ** Variance ** refers to the amount by which ˆf would change if we\n",
    "estimated it using a different training data set. Since the training data\n",
    "are used to fit the statistical learning method, different training data sets\n",
    "will result in a different ˆf. But ideally the estimate for f should not vary\n",
    "too much between training sets. However, if a method has high variance\n",
    "then small changes in the training data can result in large changes in ˆf. In\n",
    "general, more flexible statistical methods have higher variance.\n",
    "\n",
    "For Example, with linear regression through 100 data points, removing one data point would not move the function \"The line\" very much, but for a function that curves around and goes through that same 100 data points better, removing a data point could actually make it shift \"vary\" a lot.\n",
    "\n",
    "On the other hand, bias refers to the error that is introduced by approximating\n",
    "a real-life problem, which may be extremely complicated, by a much\n",
    "simpler model. For example, linear regression assumes that there is a linear\n",
    "relationship between Y and X1, X2,...,Xp.\n",
    "\n",
    "More flexible algorithms give you considerably LESS Bias. So that is the Variance/Bias tradeoff\n",
    "\n",
    "The challenge lies in finding a method for which both the variance and the squared bias are low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "The most common approach for quantifying the accuracy of our estimate ˆf is\n",
    "the training error rate, the proportion of mistakes that are made if we apply error rate\n",
    "our estimate ˆf to the training observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We continue the analogy with the estimation of the population mean μ of a random variable Y. A natural question is as follows: how accurate is the sample mean ˆμ as an estimate of μ? We have established that the average of ˆμ’s over many data sets will be very close to μ, but that a single estimate ˆμ may be a substantial underestimate or overestimate of μ. How far off will that single estimate of ˆμ be? In general, we answer this question by computing the standard error of ˆμ, written as SE(ˆμ). We have standard the well-known formula error Var(ˆμ) = SE(ˆμ) 2 = σ2 n ,\n",
    "\n",
    "where σ is the standard deviation of each of the realizations yi of Y.\n",
    "Roughly speaking, the standard error tells us the average amount that this\n",
    "estimate ˆμ differs from the actual value of μ. Equation 3.7 also tells us how\n",
    "this deviation shrinks with n—the more observations we have, the smaller\n",
    "the standard error of ˆμ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
